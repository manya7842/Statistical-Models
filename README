# NLP Building Blocks: From Scratch Implementations

A collection of fundamental NLP components implemented from scratch to understand the core mechanics behind modern language models. Each component is a self-contained implementation that demonstrates key concepts in natural language processing and deep learning.

## Components

### üìä Bigram Language Model (`bigram_model.ipynb`)
Statistical language model predicting next words using bigram probabilities.
- Text preprocessing and tokenization
- Bigram probability calculation  
- Next word prediction and model evaluation
- Understanding n-gram models and statistical text generation

### üîó Linear Layer (`linear_layer.ipynb`) 
From-scratch neural network linear transformation layer implementation.
- Forward pass and gradient computation
- Weight initialization strategies
- Tensor operations without high-level frameworks
- Building blocks of transformer architectures

### üéØ Supervised Fine-Tuning Trainer (`sftTrainer.py`)
Custom implementation for fine-tuning models on custom datasets.
- Training loop and loss computation
- Model parameter optimization for specific tasks
- Custom dataset adaptation techniques

## Purpose

These implementations bridge theory and practice by building fundamental NLP concepts from first principles. Each component is self-contained and demonstrates key concepts without relying on high-level frameworks.


## Who This Is For

Students learning NLP and deep learning fundamentals
Researchers who want to understand implementation details
Developers building custom NLP solutions
Anyone curious about how language models work internally

## ü§ù Contributing
Feel free to suggest improvements or additional components that would be valuable for understanding NLP fundamentals!
